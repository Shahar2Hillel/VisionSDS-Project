\documentclass{article}

%%%%%%% PACKAGES %%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{setspace}
\usepackage{float}
\usepackage{graphicx}
\usepackage{notoccite}
\usepackage{lscape}
\usepackage{caption}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=black]{hyperref}
\usepackage[
backend=biber,  
style=ieee,
sorting=none
]{biblatex}
\addbibresource{refs.bib}

\onehalfspace

\title{\vspace{3.5cm}\Huge{\textbf{Computer Vision \\ Surgical Applications}\\} \\\vspace{0.5cm}

\LARGE{00970222} \\
\\\vspace{0.5cm}
\LARGE{Final Project Report} \\
\\\vspace{0.5cm}


\\\vspace{3.5cm}
\LARGE{}
}
{\author{Noam Carmon \\
Raz Biton \\
Shahar Hillel \\\\}}

\date{}

\begin{document}
\pagenumbering{roman}
\maketitle
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

%%% SECTION 1 %%%
\section{Phase 1: Synthetic Data Generation}

\subsection{Data Generation Pipeline}

The synthetic pipeline generates labeled surgical tool images through scene setup, randomized lighting, tool placement, keypoint extraction, 2D projection with post-projection corrections, and annotation. The pipeline is:

\subsubsection*{Scene Setup and Object Loading}
Scenes begin in a clean Blender environment, where surgical tools are imported as 3D mesh objects. A point light is randomized in position and intensity, and HDRI environment lighting is applied with random rotation. Material properties (e.g., roughness, metallic, IOR) are varied to simulate realistic reflections and appearances.

\subsubsection*{3D Keypoint Generation (Ground Truth)}
Keypoints are computed per tool using tailored logic based on 3D mesh geometry; the method varies by tool category. \textbf{Despite minor keypoint drift, auto-labeling preserves tool structure and pose well}, so we prefer it over manual labeling.



\begin{table}[H]
\centering
\setlength{\tabcolsep}{4pt}        % tighter columns
\renewcommand{\arraystretch}{1.05} % tighter rows
\small
\caption*{Keypoint Computation (All Tools)}
\begin{tabular}{|l|l|p{6.2cm}|}
\hline
\textbf{Tool} & \textbf{Keypoint(s)} & \textbf{Computation (summary)} \\
\hline
Needle holder & top\_left, top\_right & Top 15\% by Z; choose min X (left) and max X (right). \\
\hline
Needle holder & bottom\_left, bottom\_right & Bottom 15\% by Z; choose min/max X. \\
\hline
Needle holder & middle\_left, middle\_right & Keep Z in 30--70\%; pick near centroid of left/right X-percentiles. \\
\hline
Needle holder & joint\_center & Avg of (top tips, mids); move 30\% toward middle-center; nearest vertex. \\
\hline
Tweezers & bottom\_tip & Vertex with lowest Z. \\
\hline
Tweezers & top\_left, top\_right & Split arms by median X; in each arm take top 20\% by Z, pick extreme X. \\
\hline
Tweezers & mid\_left, mid\_right & Per arm, choose vertex closest to median Z. \\
\hline
Fallback (generic) & top\_left, top\_right & Top 20\% by Z; pick min/max X. \\
\hline
Fallback (generic) & bottom\_left, bottom\_right & Bottom 20\% by Z; pick min/max X. \\
\hline
\end{tabular}
\end{table}





\subsubsection*{2D Projection}
We project the 3D keypoints to 2D using the camera intrinsics and camera poses sampled around the tools. We fix left/right labels right after projection and again at the end.\\
\textbf{Corrections:} Drop any keypoint outside the image; then clamp any keypoint outside the object's bounding box to the edge. If a point is inside the box but off the mask, move it to a plausible location (needle-holder points along side/center lines; tweezers arm points along their arms), then snap off-mask points to the nearest foreground pixel inside the box (skip this snap for the tweezers tip). For the tweezers \texttt{bottom\_tip}, only ensure it lies inside the box (clamp if needed); do not snap it onto an arm.


\subsubsection*{View-Dependent Labeling}
To maintain consistent left/right annotation across views, projected X-values are compared, and labels swapped if needed.
\subsubsection*{Partial Visibility Strategy \& Post-Projection Rules}
We induce partial visibility by sampling closer, lower camera shells and/or offsetting the look-at so tools are cropped—mimicking OR framing. After projection, we drop out-of-frame keypoints and derive a bbox from the instance mask; then we clamp bbox-out points to the edge, gently nudge any off-mask points onto the tool (skip snapping for tweezers \texttt{bottom\_tip}), and enforce left/right by X-order.

\subsubsection*{Rendering and Annotation}
We compute each object's bounding box from instance segmentation, composite the RGBA render over a photo background with a soft surgical spotlight, save the background-composited RGB image and a visualization image, and write one JSON record per image with the saved image name and, for each object, its id/category, 2D keypoints, and bounding box.




\subsection{Implemented Variations}

\subsubsection*{Instrument positioning and orientation}
We place the camera around the tools but prefer a top-down view.

\subsubsection*{Lighting conditions (intensity, color, direction)}
Each render randomizes a point light’s position (direction) and intensity. We also use an HDRI environment with random rotation and vary the world/background brightness each try.

\subsubsection*{Background variation}
Rendered frames are composited over randomly chosen photographic backgrounds.

\subsubsection*{Synthetic Images — Creative Approach 1: Photo Backgrounds, Spotlight \& Pairing}
It consists of \textbf{two creative variations}:
\begin{itemize}
    \item \textbf{Surgical-room spotlight:} Add a soft radial spotlight centered on the tools to mimic OR lighting and improve visibility.
    \item \textbf{Multi-tool images:} Render needle holder–tweezers pairs in joint top-down scenes to reflect common two-instrument frames and create realistic occlusions.
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[width=0.33\linewidth]{dataset1_1.png}\hfill
\includegraphics[width=0.33\linewidth]{dataset1_2.png}\hfill
\includegraphics[width=0.33\linewidth]{dataset1_3.png}\hfill
\includegraphics[width=0.40\linewidth]{dataset1_4.png}\hfill
\includegraphics[width=0.40\linewidth]{dataset1_5.png}
\caption{Approach 1 examples: spotlight and multi-tool pairing.}
\label{fig:approach1_examples_5}
\end{figure}


\subsubsection*{Synthetic Images — Creative Approach 2:  Operating-Room (OR) Backgrounds}
In this \textbf{creative approach} we take frames from OR videos, remove people/tools to get clean backgrounds, and composite our rendered tools on top. Then we make small changes (brightness, color, slight blur/noise, small rotations/perspective). This adds real OR context while keeping labels accurate.
\begin{figure}[H]
\centering
\includegraphics[width=0.18\linewidth]{figs/approach2_img1.jpg}\hfill
\includegraphics[width=0.18\linewidth]{figs/approach2_img2.jpg}\hfill
\includegraphics[width=0.18\linewidth]{figs/approach2_img3.jpg}\hfill
\includegraphics[width=0.18\linewidth]{figs/approach2_img4.jpg}\hfill
\includegraphics[width=0.18\linewidth]{figs/approach2_img5.jpg}
\caption{Approach 2 examples: tools on cleaned OR video backgrounds with light variations.}
\label{fig:approach2_examples_5}
\end{figure}

\subsection{Domain Gap Analysis}

\begin{table}[H]
\centering

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{0.22\textwidth}|p{0.24\textwidth}|p{0.24\textwidth}|p{0.24\textwidth}|}
\hline
\textbf{Aspect} & \textbf{Synthetic} & \textbf{Real} & \textbf{Domain Gap} \\
\hline
Tool Usage Context & Tools are alone and not used by hands. & Tools are held by people and touch tissue. & No hands or actions shown. \\
\hline
Background Semantics & Backgrounds are generic, not medical. & Surgical room environment. & Missing medical setting cues. \\
\hline
Texture and Surface Realism & Surfaces look clean and perfect. & There are reflections, scratches, stains, fluids. & Too perfect compared to real life. \\
\hline
Scene Complexity and Occlusion & Simple scenes with few overlaps. & Crowded scenes with hands and tools overlapping. & Less clutter makes training easier than it should be. \\
\hline
\end{tabular}
\end{table}



\subsection{Discussion of Challenges and Key Findings}

\subsubsection*{Challenges}
- \textbf{Annotation Strategy:} \newline Manual labeling was unscalable; automation required mesh understanding and consistent landmark definitions.  \newline
- \textbf{Ground Truth Modeling:} \newline Bounding boxes lacked precision; keypoints added detail but required careful design.  \newline
- \textbf{Domain Alignment:} \newline Synthetic scenes missed surgical cues like hands, lighting, and tissue.

\subsubsection*{Key Findings}
- \textbf{Reliable Auto-Annotation:} \newline Ground truth labels were generated using mesh metadata—accurate and scalable.  \newline
- \textbf{Multi-Tool and Multi-Pose Scenes:} \newline Improved realism and contextual diversity.  \newline
- \textbf{Spotlight Lighting:} \newline Enhanced tool visibility and mimicked surgical illumination.  \newline
- \textbf{Pose and Camera Variation:} \newline Supported diverse perspectives and improved generalization.

%%% SECTION 2 %%%
\section{Section 2}

%%% SECTION 3 %%%
\section{Section 3}

%%% END %%%
\setstretch{1} % Bibliography spacing
\end{document}
